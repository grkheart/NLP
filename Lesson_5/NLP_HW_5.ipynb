{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRL86Xl29r9W"
   },
   "source": [
    "# Part-of-Speech разметка, NER, извлечение отношений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xNMViGM9wc3"
   },
   "source": [
    "## Задание 1. Написать теггер на данных с русским языком\n",
    "\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3. сравнить все реализованные методы сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjrfjVzk97DW",
    "outputId": "5955826d-d2e2-4b32-a6e1-2af323b50c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyconll\r\n",
      "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: pyconll\r\n",
      "Successfully installed pyconll-3.1.0\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RxVPSR9a97GA"
   },
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3V0kTd897Ie",
    "outputId": "1c164926-a634-45ff-94fd-8b636f4e07cd"
   },
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeTx4V4197LY",
    "outputId": "f772c6ea-e573-4b1e-d563-bac7532a38fc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer, HashingVectorizer, TfidfVectorizer\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "from nltk.corpus import names\n",
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Co5pGBIcC37m"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUFRqelQONR5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxTBkmfe-zgj"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3SKrvSNB-zjQ",
    "outputId": "a42f31a0-cb36-4bb5-c948-a30479e180eb"
   },
   "outputs": [],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npHB-tXf_VJk"
   },
   "source": [
    "### 1.1 проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey6z-oUI-zqw",
    "outputId": "4a8fffb2-4f7e-4912-e8c2-964b1a2e2e6a"
   },
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])\n",
    "    \n",
    "    \n",
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSETqemP-zvg",
    "outputId": "7a6bf5dc-28b4-407b-c889-b659fe98e731"
   },
   "outputs": [],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7Ke_BhNA_-Q",
    "outputId": "02ad52a3-65be-4dc5-80ec-4e836271354a"
   },
   "outputs": [],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qB8wa_-lBABS",
    "outputId": "b05f8757-3468-4fcb-b31c-1b248d8a18d9"
   },
   "outputs": [],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBASDm92BADc",
    "outputId": "f48f4ec3-4dde-441d-e010-3b3c43ad45ca"
   },
   "outputs": [],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vKQ2JcuBAF8",
    "outputId": "f3ea3d0d-24ca-41c8-fb80-c59c289ed48c"
   },
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NOUN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [\n",
    "                      UnigramTagger, \n",
    "                      BigramTagger, \n",
    "                      TrigramTagger\n",
    "                     ],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwThLuSXCmWp"
   },
   "source": [
    "## 1.2 написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhlaNsDWBAIO",
    "outputId": "10bd1a8c-a1ec-4c5b-9f77-a6f20e491912"
   },
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "        \n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label) \n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAiXtSRYBAK0",
    "outputId": "929c2d7d-21de-4512-effc-8fa3bd2c96ff"
   },
   "outputs": [],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tU01r_C5BAMt",
    "outputId": "727ab69e-bc5f-440f-f0ba-2929646f7816"
   },
   "outputs": [],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SpEa4TaCuhf",
    "outputId": "02a728da-0929-40ef-82ac-edb2c2cb3868"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "\n",
    "X_train_1 = coder_1.fit_transform(train_tok)\n",
    "X_test_1 = coder_1.transform(test_tok)\n",
    "\n",
    "X_train_2 = coder_2.fit_transform(train_tok)\n",
    "X_test_2 = coder_2.transform(test_tok)\n",
    "\n",
    "\n",
    "X_train = hstack((X_train_1,X_train_2))\n",
    "X_test = hstack((X_test_1,X_test_2))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)    \n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print('TfidfVectorizer_char + HashingVectorizer_word :', accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuUHDflfNggP"
   },
   "source": [
    "__Выводы:__\n",
    "\n",
    "Для nltk.tag лучший вариант это: Комбинация из DefaultTagger UnigramTagger BigramTagger TrigramTagger\n",
    "0.9119991237825633\n",
    "\n",
    "Для Vectorizer лучший вариант это: Комбинация из LogisticRegression поверх TfidfVectorizer при условии analyzer='char'\n",
    "0.9487749806221144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxThe_cF9wR3"
   },
   "source": [
    "## Задание 2. Проверить насколько хорошо работает NER\n",
    "данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы:\n",
    " - передаём в сетку токен и его соседей\n",
    " - передаём в сетку только токен\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1q-UfUCT1um"
   },
   "source": [
    "## 2.1 взять нер из nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMSLGJad9rG7",
    "outputId": "e072af48-9038-4a0e-e960-b22ef61b65b3"
   },
   "outputs": [],
   "source": [
    "!pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtjCVUsmOWWX"
   },
   "outputs": [],
   "source": [
    "import corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myTuCMoeO6Em"
   },
   "outputs": [],
   "source": [
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FipubRb7OWZJ",
    "outputId": "65df0ff4-2c27-48c7-ec94-b2f7be78cff4"
   },
   "outputs": [],
   "source": [
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0Ino_5YQJF1",
    "outputId": "eeb6467f-586c-4e4c-b77f-417f4f2a5dd4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcUTAyvKS2Me",
    "outputId": "2fd4e787-f01e-4641-a491-f80032167a2c"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSbtz49EQJIy"
   },
   "outputs": [],
   "source": [
    "path = 'Collection5/'\n",
    "records = load_ne5(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L3X0IzISKgn"
   },
   "outputs": [],
   "source": [
    "document = next(records).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "jgRhwYwjSlsL",
    "outputId": "b52d9ff4-e158-4a08-b90d-acc97e07b445"
   },
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbMfR0rASlut",
    "outputId": "0a70c7fe-5cb4-4798-e624-c1b50df47d1e"
   },
   "outputs": [],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJt3uMd7T47m"
   },
   "source": [
    "## 2.2. проверить deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "log7mU1VSlxg"
   },
   "outputs": [],
   "source": [
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не пошла установка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYu_1z9OUZF4"
   },
   "source": [
    "## 2.3 написать свой нер попробовать разные подходы:\n",
    "- передаём в сетку токен и его соседей\n",
    "- передаём в сетку только токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FR2sPTFOVX2h",
    "outputId": "0848fd0c-15eb-465a-903c-769bb7eb2589"
   },
   "outputs": [],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMhyzUlVSl2Y"
   },
   "outputs": [],
   "source": [
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAJjv18i6QWk"
   },
   "outputs": [],
   "source": [
    "records = corus.load_ne5(path)\n",
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "       \n",
    "        result = 'None'        \n",
    "        \n",
    "        for item in rec.spans:            \n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'PER'):\n",
    "                result = 'PER'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'ORG'):\n",
    "                result = 'ORG'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'MEDIA'):\n",
    "                result = 'MEDIA'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'LOC'):\n",
    "                result = 'LOC'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'GEOPOLIT'):\n",
    "                result = 'GEOPOLIT'\n",
    "                break\n",
    "                \n",
    "    \n",
    "        words.append([token.text, result])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k8RAMja6QZo"
   },
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyYYKXfp6QcR",
    "outputId": "58e6441d-ab6c-48b2-c020-3405ceeb622f"
   },
   "outputs": [],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2IIOlMJW6Qey",
    "outputId": "1b1f8585-a2f8-423c-997f-d86a9ae76365"
   },
   "outputs": [],
   "source": [
    "df_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBnCTkAL6QhS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdVw5yXg6QjJ"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJjCF01R6QlO"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcJOq4vxWc6r"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xt16YZCWc9H"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "        return input_data\n",
    "\n",
    "def data_prep(train_data, seq_len=1, vocab_size = 30000):    \n",
    "    \n",
    "    vocab_size = 30000\n",
    "    #seq_len = 1\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "    # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "    text_data = train_data.map(lambda x, y: x)\n",
    "    vectorize_layer.adapt(text_data)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htTHBq75Vpyy"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(len(df_words['tag'].value_counts()), activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        return self.fc3(concat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3nZoESDVp1X",
    "outputId": "53a7da33-b24f-4a9a-a100-3fb2de36f6e4"
   },
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 1, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVG4rNAZVp4G",
    "outputId": "d16efab9-9c8b-47df-807d-bd4d5ed4437e"
   },
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 3, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG6xCuD_a8TP"
   },
   "source": [
    "#### Вывод.\n",
    "\n",
    "Длина последовательности практически не влияет на результат"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}